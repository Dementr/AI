{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb127136",
   "metadata": {},
   "source": [
    "# Лабораторная работа 7\n",
    "\n",
    "   по теме\n",
    "                 \n",
    "   **Классификация обзоров фильмов**\n",
    "   \n",
    "   ****\n",
    "\n",
    "   Выполнил студент\n",
    "\n",
    "   Группы БСТ1801\n",
    "\n",
    "   Харатишвили Заза"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0a20c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from pydash import *\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense, Dropout, Flatten, Embedding, LSTM, Conv1D, MaxPooling1D, GlobalMaxPooling1D, BatchNormalization, Average, Input, GlobalAveragePooling2D, Activation\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4146de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ограничим набор данных до 5000 слов\n",
    "top_words = 5000\n",
    "# Устанавливаем максимальную длину\n",
    "max_review_length = 500\n",
    "# Вектор длины представления каждого слова\n",
    "embedding_vector_length = 32\n",
    "# Количество эпох\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ee737bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем датасет\n",
    "from keras.datasets import imdb\n",
    "\n",
    "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=top_words)\n",
    "# Обьединяем данные в доль одной оси\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "targets = np.concatenate((training_targets, testing_targets), axis=0)\n",
    "\n",
    "input_shape = X_train[0,:].shape\n",
    "inp = Input(shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9e6d7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Дополняем или сокращаем обзоры чтобы они были одинаковой длины\n",
    "x_train = sequence.pad_sequences(training_data, maxlen=max_review_length)\n",
    "x_test = sequence.pad_sequences(testing_data, maxlen=max_review_length)\n",
    "\n",
    "y_train = training_targets\n",
    "y_test = testing_data\n",
    "\n",
    "x_test = np.asarray(x_train).astype(np.float32)\n",
    "y_test = np.asarray(y_train).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6f6abc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_29 (Embedding)    (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " lstm_29 (LSTM)              (None, 100)               53200     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 192s 488ms/step - loss: 0.4435 - accuracy: 0.7862 - val_loss: 1.0073 - val_accuracy: 0.4992\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 188s 482ms/step - loss: 0.2959 - accuracy: 0.8819 - val_loss: 1.4116 - val_accuracy: 0.4982\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 187s 480ms/step - loss: 0.2399 - accuracy: 0.9072 - val_loss: 1.7731 - val_accuracy: 0.4994\n",
      "Accuracy: 93.33%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Embedding получает номера слов, а на выходе выдаёт их векторные представления\n",
    "# Индекс слова не больше top_words-1\n",
    "# Размерность векторного пространства embedding_vecor_length\n",
    "# Число входов = max_review_length\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "# 100 нейронов (умных единиц памяти)\n",
    "model.add(LSTM(100))\n",
    "# На выходе 0 или 1, тк задача классификации\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(X_test, y_test), epochs=num_epochs, batch_size=64)\n",
    "# Оценим качество модели по тестовым данным\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "47c4c7a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_30 (Embedding)    (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " conv1d_40 (Conv1D)          (None, 500, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d_16 (MaxPoolin  (None, 250, 32)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " lstm_30 (LSTM)              (None, 100)               53200     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 91s 230ms/step - loss: 0.4146 - accuracy: 0.7916 - val_loss: 1.8072 - val_accuracy: 0.4991\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 90s 232ms/step - loss: 0.2416 - accuracy: 0.9057 - val_loss: 1.3774 - val_accuracy: 0.4965\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 0.1973 - accuracy: 0.9242 - val_loss: 1.8938 - val_accuracy: 0.4974\n",
      "Accuracy: 94.73%\n"
     ]
    }
   ],
   "source": [
    "# Добавим свертывающий слой и слой пула\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(X_test, y_test), epochs=num_epochs, batch_size=64)\n",
    "# Оценим качество модели по тестовым данным\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80b5a10",
   "metadata": {},
   "source": [
    "Для того, чтобы провести ансамблирование моделей выведем модели в функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6ddfedec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция компиляции и обучения\n",
    "def compile_and_train(model, num_epochs): \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    filepath = model.name + '.{epoch:02d}-{loss:.2f}.hdf5'\n",
    "    # Сохраняем модель\n",
    "    checkpoint = ModelCheckpoint(filepath, \n",
    "                                 monitor='loss',  # Отслеживаем метрики потери\n",
    "                                 verbose=0, \n",
    "                                 save_weights_only=True, # Сохраняем только веса модели\n",
    "                                 save_best_only=True, # Сохраняем только лучшую модель\n",
    "                                 mode='auto', period=1)\n",
    "    history = model.fit(x=x_train, y=y_train, \n",
    "                        batch_size=32, epochs=num_epochs, \n",
    "                        verbose=1, callbacks=[checkpoint], \n",
    "                        validation_split=0.2)\n",
    "    \n",
    "    weight_files = glob.glob(os.path.join(os.getcwd(), '*'))\n",
    "    weight_file = max(weight_files, key=os.path.getctime)\n",
    "    \n",
    "    print(model.evaluate(X_test, testing_targets, verbose=0))\n",
    "    return history, weight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "54d3d92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(model_input):\n",
    "\n",
    "    emb = Embedding(top_words, embedding_vector_length)(model_input)\n",
    "    lstm = LSTM(128)(emb)\n",
    "    drop = Dropout(0.5)(lstm)\n",
    "    dense = Dense(1)(drop)\n",
    "\n",
    "    model_out = Activation(activation='sigmoid')(dense)\n",
    "    \n",
    "    model = Model(model_input, model_out, name='lstm')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_v1 = lstm(Input(shape=x_train[0,:].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7a7c55cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/3\n",
      "625/625 [==============================] - 103s 162ms/step - loss: 0.4891 - accuracy: 0.7631 - val_loss: 0.3835 - val_accuracy: 0.8338\n",
      "Epoch 2/3\n",
      "625/625 [==============================] - 102s 163ms/step - loss: 0.3211 - accuracy: 0.8705 - val_loss: 0.3204 - val_accuracy: 0.8610\n",
      "Epoch 3/3\n",
      "625/625 [==============================] - 103s 164ms/step - loss: 0.2749 - accuracy: 0.8910 - val_loss: 0.3146 - val_accuracy: 0.8666\n",
      "[0.31858593225479126, 0.8708800077438354]\n"
     ]
    }
   ],
   "source": [
    "_, lstm_weight_file = compile_and_train(model_v1, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1577eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(model_input):\n",
    "\n",
    "    emb = Embedding(top_words, embedding_vector_length, input_length=max_review_length)(model_input)\n",
    "\n",
    "    conv1d_1 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')(emb)\n",
    "    conv1d_2 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')(conv1d_1)\n",
    "    conv1d_3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')(conv1d_2)\n",
    "    mp1d_1 = MaxPooling1D(pool_size=2)(conv1d_3)\n",
    "    drop_1 = Dropout(0.4)(mp1d_1)\n",
    "\n",
    "    conv1d_4 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')(drop_1)\n",
    "    conv1d_5 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')(conv1d_4)\n",
    "    conv1d_6 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')(conv1d_5)\n",
    "    mp1d_2 = MaxPooling1D(pool_size=2)(conv1d_6)\n",
    "    drop_2 = Dropout(0.4)(mp1d_2)\n",
    "    lstm = LSTM(100)(drop_2)\n",
    "    dense = Dense(1)(lstm)\n",
    "\n",
    "    model_out = Activation(activation='sigmoid')(dense)\n",
    "    \n",
    "    model = Model(model_input, model_out, name='cnn')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_v2 = cnn(Input(shape=x_train[0,:].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9d6269e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/3\n",
      "625/625 [==============================] - 102s 162ms/step - loss: 0.5103 - accuracy: 0.7048 - val_loss: 0.3528 - val_accuracy: 0.8338\n",
      "Epoch 2/3\n",
      "625/625 [==============================] - 96s 154ms/step - loss: 0.2640 - accuracy: 0.8947 - val_loss: 0.2902 - val_accuracy: 0.8794\n",
      "Epoch 3/3\n",
      "625/625 [==============================] - 99s 159ms/step - loss: 0.2070 - accuracy: 0.9205 - val_loss: 0.2799 - val_accuracy: 0.8836\n",
      "[0.2829585671424866, 0.885640025138855]\n"
     ]
    }
   ],
   "source": [
    "_, cnn_weight_file = compile_and_train(model_v2, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a83a6d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_WEIGHT_FILE = os.path.join(cnn_weight_file)\n",
    "LSTM_WEIGHT_FILE = os.path.join(lstm_weight_file)\n",
    "\n",
    "# Получаем модели\n",
    "cnn_model = cnn(inp)\n",
    "lstm_model = lstm(inp)\n",
    "\n",
    "# Загружаем сохраненные веса моделей\n",
    "cnn_model.load_weights(CNN_WEIGHT_FILE)\n",
    "lstm_model.load_weights(LSTM_WEIGHT_FILE)\n",
    "\n",
    "# Обьединяем модели в один массив\n",
    "models = [cnn_model, lstm_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "95c5028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ansemble(models, model_input):\n",
    "    # Получаем результаты моделей\n",
    "    outputs = [model.outputs[0] for model in models]\n",
    "    # Усредняем\n",
    "    y = Average()(outputs)\n",
    "    # Записываем в одну модель усредненные результаты работы моделей\n",
    "    model = Model(model_input, y, name='ensemble')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0d5cd168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "391/391 [==============================] - 490s 1s/step - loss: 0.1866 - accuracy: 0.9334 - val_loss: 0.1592 - val_accuracy: 0.9326\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 503s 1s/step - loss: 0.1582 - accuracy: 0.9470 - val_loss: 0.1454 - val_accuracy: 0.9631\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 506s 1s/step - loss: 0.1324 - accuracy: 0.9560 - val_loss: 0.0989 - val_accuracy: 0.9721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c4c97140a0>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Обучаем модель, полученную после ансамблирования моделей\n",
    "e_model = ansemble(models, inp)\n",
    "e_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "e_model.fit(np.asarray(x_train).astype('float32'),\n",
    "            np.asarray(y_train).astype('float32'),\n",
    "            validation_data=(np.asarray(x_test).astype('float32'), np.asarray(y_train).astype('float32')),\n",
    "            epochs=num_epochs, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8dbfc1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Получаем индексы из датасета\n",
    "indexes = imdb.get_word_index()\n",
    "\n",
    "# Функция, позволяющая загружать пользовательский текст и получать результат ансабля сетей\n",
    "def user_text_predict(text, model):\n",
    "    text_vec = np.zeros((1, max_review_length)) # Заполняем до max_review_length одномерный вектор\n",
    "    # Преобразуем текст\n",
    "    for idx, word in enumerate(text.translate(str.maketrans('', '', string.punctuation)).lower().split(' ')):\n",
    "        # Если индексы больше 10000 или None присваиваем 0\n",
    "        if idx < 10000 or idx is None and word in indexes:\n",
    "            text_vec[0][idx] = indexes[word]\n",
    "    tps = sequence.pad_sequences(text_vec, maxlen=max_review_length)\n",
    "    print (model.predict(tps))\n",
    "    return np.around(model.predict(tps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3c60645c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5583335]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_text_predict('The best movie I have ever seen', e_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39099aff",
   "metadata": {},
   "source": [
    "## Вывод\n",
    "\n",
    "В результате выполнения лабораторной работы было проведено ансамблирование моделей, благодаря которому значительно повысилась точность работы. Была написана функция, позволяющая загружать пользовательский текст и получать результат ансабля сетей, и провели тестирование сетей на основе загруженного текста."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
