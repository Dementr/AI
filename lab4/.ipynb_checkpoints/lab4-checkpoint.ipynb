{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9b00585",
   "metadata": {},
   "source": [
    "# Лабораторная работа 4\n",
    "\n",
    "   по теме\n",
    "                 \n",
    "   **Распознавание рукописных символов**\n",
    "   \n",
    "   ****\n",
    "\n",
    "   Выполнил студент\n",
    "\n",
    "   Группы БСТ1801\n",
    "\n",
    "   Харатишвили Заза"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6413b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Импорт бибилиотек и методов\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b79e1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Получение данных\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels),(test_images, test_labels) = mnist.load_data()\n",
    "#train - тренировочные наборы для обучения\n",
    "#test - контрольный набор для тестирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76437de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#Сравнение текстового изображения с его меткой для проверки корректности\n",
    "plt.imshow(train_images[1],cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "print(train_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43765928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Нормализуем входные данные путем преобразования изоображения в массивы в бинарный интервал [0,1] из [0;255]\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb17d26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "#Закодируем метки категорий - Переведем изображения в унитарный код\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "print (test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0aae0776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 1s 919us/step - loss: 1.5837 - accuracy: 0.6275\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 0s 900us/step - loss: 0.6847 - accuracy: 0.8386\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 0s 894us/step - loss: 0.4759 - accuracy: 0.8779\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 0s 889us/step - loss: 0.3972 - accuracy: 0.8925\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 0s 890us/step - loss: 0.3578 - accuracy: 0.8998\n",
      "Test data loss: 0.3327396512031555\n",
      "Test data accuracy: 0.9089000225067139\n",
      "Train data loss: 0.3422086834907532\n",
      "Train data accuracy: 0.9052166938781738\n"
     ]
    }
   ],
   "source": [
    "#Задаем базовую архитектуру сети\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#Настраиваем параметры сети \n",
    "# Функция потерь: Категориальная перекрестная энтропия - для оценки точности предсказаний\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Начинаем обучение сети\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "#Проверяем как модель распознает тестовый набор\n",
    "test_loss, test_accuracy = model.evaluate(\n",
    "    test_images, test_labels, verbose=0)\n",
    "print('Test data loss:', test_loss)\n",
    "print('Test data accuracy:', test_accuracy)\n",
    "#Проверяем как модель распознает контрольный набор\n",
    "train_loss, train_accuracy = model.evaluate(\n",
    "    train_images, train_labels, verbose=0)\n",
    "print('Train data loss:', train_loss)\n",
    "print('Train data accuracy:', train_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7ebc65",
   "metadata": {},
   "source": [
    "Найдем архитектуру сети, точность классификации которой будет не менее 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "427f247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Получение данных\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(new_train_images, new_train_labels),(new_test_images, new_test_labels) = mnist.load_data()\n",
    "#train - тренировочные наборы для обучения\n",
    "#test - контрольный набор для тестирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21548931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Нормализуем входные данные путем преобразования изоображения в массивы в бинарный интервал [0,1] из [0;255]\n",
    "new_train_images = new_train_images / 255.0\n",
    "new_test_images = new_test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7951dec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "#Конвертируем из 1 измерения в 10 измерений\n",
    "new_train_labels = to_categorical(new_train_labels)\n",
    "new_test_labels = to_categorical(new_test_labels)\n",
    "print (test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38538e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2214 - accuracy: 0.9347\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0942 - accuracy: 0.9712\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0683 - accuracy: 0.9786\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0533 - accuracy: 0.9827\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0426 - accuracy: 0.9863\n",
      "Test data loss: 0.06452581286430359\n",
      "Test data accuracy: 0.982200026512146\n",
      "Train data loss: 0.019632963463664055\n",
      "Train data accuracy: 0.9938333630561829\n"
     ]
    }
   ],
   "source": [
    "#Задаем базовую архитектуру сети\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2)) #Случайно отключает нейроны в фазе обучения\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#Настраиваем параметры сети\n",
    "# SGD - 0.2/0.94\n",
    "# RMSprop - 0.035/0.99\n",
    "# Adagrad - 0.35/0.90\n",
    "# Adadelta - 1.15/0.79\n",
    "# Adam - 0.021/0.99\n",
    "# Adamax - 0.06/0.98\n",
    "# Nadam - 0.022/0.99\n",
    "model.compile(optimizer='Nadam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Начинаем обучение сети\n",
    "history = model.fit(new_train_images, new_train_labels, epochs=5, batch_size=32)\n",
    "\n",
    "#Проверяем как модель распознает тестовый набор\n",
    "test_loss, test_accuracy = model.evaluate(\n",
    "    new_test_images, new_test_labels, verbose=0)\n",
    "print('Test data loss:', test_loss)\n",
    "print('Test data accuracy:', test_accuracy)\n",
    "#Проверяем как модель распознает контрольный набор\n",
    "train_loss, train_accuracy = model.evaluate(\n",
    "    new_train_images, new_train_labels, verbose=0)\n",
    "print('Train data loss:', train_loss)\n",
    "print('Train data accuracy:', train_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0c18e9",
   "metadata": {},
   "source": [
    "Напишем функцию, которая позволит загружать пользовательское изображение не из датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7e96dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Импорт бибилиотек\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "#from io import BytesIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e375a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Зададим размеры изображения и получим произведения элементов для reshape\n",
    "img_size = (28, 28)\n",
    "img_size_flat = np.prod(img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7ddf0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим данные\n",
    "(train_images, train_labels),(test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Переведем данные в формат с плавающей запятой, поскольку они являютстя целыми числами,\n",
    "#изменим форму матрицы, тоесть перенесем в одномерное пространство,тк рассматриваем каждый пиксель как\n",
    "#отдельное входное значение. И проведем нормализацию, поделив на 255\n",
    "train_images = train_images.reshape(-1, img_size_flat).astype('float32') / 255\n",
    "test_images = test_images.reshape(-1, img_size_flat).astype('float32') / 255\n",
    "\n",
    "# Переведем изображения в унитарный код\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "619c94d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.3116 - accuracy: 0.9143 - val_loss: 0.1625 - val_accuracy: 0.9543\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 0s 944us/step - loss: 0.1341 - accuracy: 0.9621 - val_loss: 0.1109 - val_accuracy: 0.9679\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 0s 953us/step - loss: 0.0912 - accuracy: 0.9735 - val_loss: 0.0941 - val_accuracy: 0.9709\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 0s 970us/step - loss: 0.0676 - accuracy: 0.9807 - val_loss: 0.0775 - val_accuracy: 0.9774\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 0s 972us/step - loss: 0.0531 - accuracy: 0.9847 - val_loss: 0.0705 - val_accuracy: 0.9786\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 0s 930us/step - loss: 0.0414 - accuracy: 0.9882 - val_loss: 0.0652 - val_accuracy: 0.9789\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 0s 937us/step - loss: 0.0321 - accuracy: 0.9912 - val_loss: 0.0711 - val_accuracy: 0.9777\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 0s 966us/step - loss: 0.0262 - accuracy: 0.9931 - val_loss: 0.0651 - val_accuracy: 0.9789\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 0s 959us/step - loss: 0.0210 - accuracy: 0.9947 - val_loss: 0.0686 - val_accuracy: 0.9802\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 0s 953us/step - loss: 0.0171 - accuracy: 0.9958 - val_loss: 0.0639 - val_accuracy: 0.9808\n",
      "Test data loss: 0.063894122838974\n",
      "Test data accuracy: 0.9807999730110168\n",
      "Train data loss: 0.013075251132249832\n",
      "Train data accuracy: 0.9970999956130981\n"
     ]
    }
   ],
   "source": [
    "#Задаем базовую архитектуру сети\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#Настраиваем параметры сети\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Начинаем обучение сети\n",
    "model.fit(train_images, train_labels, epochs=10, batch_size=128, validation_data=(test_images, test_labels))\n",
    "\n",
    "#Проверяем как модель распознает тестовый набор\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=0)\n",
    "print('Test data loss:', test_loss)\n",
    "print('Test data accuracy:', test_accuracy)\n",
    "#Проверяем как модель распознает контрольный набор\n",
    "train_loss, train_accuracy = model.evaluate(train_images, train_labels, verbose=0)\n",
    "print('Train data loss:', train_loss)\n",
    "print('Train data accuracy:', train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b11d4b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция получения изображения и его преобразования для дальнейшей работы\n",
    "# url - ссылка на изображение\n",
    "# target_size - размер изображения, преобразуем до 28х28 пикселей\n",
    "# convert_to='L' - преобразование цвета изображения на оттенки серого\n",
    "def get_image(url, target_size=(img_size), convert_to='L'):\n",
    "#     if url.lower().startswith('http'):\n",
    "#         r = requests.get(url)\n",
    "#         url = BytesIO(r.content)\n",
    "    img = (Image.open(url).convert(convert_to).resize(target_size))\n",
    "    return np.array(img).reshape(-1, np.prod(target_size))\n",
    "\n",
    "def predict_digit(img, model):\n",
    "    return model.predict(img).argmax() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc3ecde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Загрузим свою картинку и подадим ее на вход обученной модели\n",
    "img = get_image(r\"C:\\Users\\zazah\\sii\\lab4\\3.png\")\n",
    "#Узнаем предсказание для загруженного нами изображения в уже обученную модель\n",
    "predict = predict_digit(img, model)\n",
    "plt.imshow(img.reshape(img_size), cmap='gray')\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fc18d1",
   "metadata": {},
   "source": [
    "## Вывод\n",
    "\n",
    "В результате выполнения лабораторной работы можно сделать вывод, что для задачи анализа изображения наиболее эффективен\n",
    "анализатор Adam, при использовании других оптимизаторов потери возрастают, а точность снижается. Также была реализована функция, которая выполняет загрузку пользовательского изображения с компьютера пользователя"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
