{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb127136",
   "metadata": {},
   "source": [
    "# Лабораторная работа 7\n",
    "\n",
    "   по теме\n",
    "                 \n",
    "   **Классификация обзоров фильмов**\n",
    "   \n",
    "   ****\n",
    "\n",
    "   Выполнил студент\n",
    "\n",
    "   Группы БСТ1801\n",
    "\n",
    "   Харатишвили Заза"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a20c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Model, Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from pydash import *\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense, Dropout, Flatten, Embedding, LSTM, Conv1D, MaxPooling1D, GlobalMaxPooling1D, BatchNormalization, Average, Input, GlobalAveragePooling2D, Activation\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4146de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ограничим набор данных до 5000\n",
    "top_words = 5000\n",
    "# Устанавливаем максимальную длину\n",
    "max_review_length = 500\n",
    "# Вектор длины представления каждого слова\n",
    "embedding_vector_length = 32\n",
    "# Количество эпох\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee737bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем датасет\n",
    "from keras.datasets import imdb\n",
    "\n",
    "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=top_words)\n",
    "# Обьединяем данные в доль одной оси\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "targets = np.concatenate((training_targets, testing_targets), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e6d7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Дополняем или сокращаем обзоры чтобы они были одинаковой длины\n",
    "x_train = sequence.pad_sequences(training_data, maxlen=max_review_length)\n",
    "x_test = sequence.pad_sequences(testing_data, maxlen=max_review_length)\n",
    "\n",
    "y_train = training_targets\n",
    "y_test = testing_data\n",
    "\n",
    "x_test = np.asarray(x_train).astype(np.float32)\n",
    "y_test = np.asarray(y_train).astype(np.float32)\n",
    "\n",
    "# Задаем input_shape\n",
    "input_shape = x_train[0,:].shape\n",
    "# Входной слой модели\n",
    "inp = Input(shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f6abc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 140s 355ms/step - loss: 0.4943 - accuracy: 0.7633 - val_loss: 0.3145 - val_accuracy: 0.8790\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 145s 372ms/step - loss: 0.3070 - accuracy: 0.8772 - val_loss: 0.2282 - val_accuracy: 0.9182\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 148s 379ms/step - loss: 0.2491 - accuracy: 0.9046 - val_loss: 0.2668 - val_accuracy: 0.9015\n",
      "Accuracy: 90.15%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Embedding получает номера слов, а на выходе выдаёт их векторные представления\n",
    "# Индекс слова не больше top_words-1\n",
    "# Размерность векторного пространства embedding_vecor_length\n",
    "# Число входов = max_review_length\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "# 100 нейронов (умных единиц памяти)\n",
    "model.add(LSTM(100))\n",
    "# На выходе 0 или 1, тк задача классификации\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=num_epochs, batch_size=64)\n",
    "# Оценим качество модели по тестовым данным\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47c4c7a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 500, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 250, 32)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 63s 159ms/step - loss: 0.4145 - accuracy: 0.8001 - val_loss: 0.2318 - val_accuracy: 0.9157\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 0.2371 - accuracy: 0.9072 - val_loss: 0.1698 - val_accuracy: 0.9382\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.1972 - accuracy: 0.9244 - val_loss: 0.1372 - val_accuracy: 0.9566\n",
      "Accuracy: 95.66%\n"
     ]
    }
   ],
   "source": [
    "# Добавим свертывающий слой и слой пула\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=num_epochs, batch_size=64)\n",
    "# Оценим качество модели по тестовым данным\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80b5a10",
   "metadata": {},
   "source": [
    "Для того, чтобы провести ансамблирование моделей выведем модели в функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ddfedec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция компиляции и обучения\n",
    "def compile_and_train(model, num_epochs): \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    filepath = model.name + '.{epoch:02d}-{loss:.2f}.hdf5'\n",
    "    # Сохраняем модель\n",
    "    checkpoint = ModelCheckpoint(filepath, \n",
    "                                 monitor='loss',  # Отслеживаем метрики потери\n",
    "                                 verbose=0, \n",
    "                                 save_weights_only=True, # Сохраняем только веса модели\n",
    "                                 save_best_only=True, # Сохраняем только лучшую модель\n",
    "                                 mode='auto', period=1)\n",
    "    history = model.fit(x=x_train, y=y_train, \n",
    "                        batch_size=32, epochs=num_epochs, \n",
    "                        verbose=1, callbacks=[checkpoint], \n",
    "                        validation_split=0.2)\n",
    "    \n",
    "    weight_files = glob.glob(os.path.join(os.getcwd(), '*'))\n",
    "    weight_file = max(weight_files, key=os.path.getctime)\n",
    "    \n",
    "    print(model.evaluate(x_test, testing_targets, verbose=0))\n",
    "    return history, weight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54d3d92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(model_input):\n",
    "\n",
    "    emb = Embedding(top_words, embedding_vector_length)(model_input)\n",
    "    lstm = LSTM(128)(emb)\n",
    "    drop = Dropout(0.5)(lstm)\n",
    "    dense = Dense(1)(drop)\n",
    "    model_out = Activation(activation='sigmoid')(dense)\n",
    "    \n",
    "    model = Model(model_input, model_out, name='lstm')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_v1 = lstm(Input(shape=x_train[0,:].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a7c55cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/3\n",
      "625/625 [==============================] - 90s 142ms/step - loss: 0.4896 - accuracy: 0.7631 - val_loss: 0.3477 - val_accuracy: 0.8568\n",
      "Epoch 2/3\n",
      "625/625 [==============================] - 90s 144ms/step - loss: 0.3550 - accuracy: 0.8497 - val_loss: 0.3767 - val_accuracy: 0.8402\n",
      "Epoch 3/3\n",
      "625/625 [==============================] - 89s 143ms/step - loss: 0.2776 - accuracy: 0.8895 - val_loss: 0.3628 - val_accuracy: 0.8388\n",
      "[1.4356050491333008, 0.49851998686790466]\n"
     ]
    }
   ],
   "source": [
    "_, lstm_weight_file = compile_and_train(model_v1, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1577eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(model_input):\n",
    "\n",
    "    emb = Embedding(top_words, embedding_vector_length, input_length=max_review_length)(model_input)\n",
    "\n",
    "    conv1d_1 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')(emb)\n",
    "    conv1d_2 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')(conv1d_1)\n",
    "    conv1d_3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')(conv1d_2)\n",
    "    mp1d_1 = MaxPooling1D(pool_size=2)(conv1d_3)\n",
    "    drop_1 = Dropout(0.4)(mp1d_1)\n",
    "\n",
    "    conv1d_4 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')(drop_1)\n",
    "    conv1d_5 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')(conv1d_4)\n",
    "    conv1d_6 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')(conv1d_5)\n",
    "    mp1d_2 = MaxPooling1D(pool_size=2)(conv1d_6)\n",
    "    drop_2 = Dropout(0.4)(mp1d_2)\n",
    "    lstm = LSTM(100)(drop_2)\n",
    "    dense = Dense(1)(lstm)\n",
    "\n",
    "    model_out = Activation(activation='sigmoid')(dense)\n",
    "    \n",
    "    model = Model(model_input, model_out, name='cnn')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_v2 = cnn(Input(shape=x_train[0,:].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d6269e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/3\n",
      "625/625 [==============================] - 65s 103ms/step - loss: 0.6933 - accuracy: 0.5027 - val_loss: 0.6935 - val_accuracy: 0.4938\n",
      "Epoch 2/3\n",
      "625/625 [==============================] - 64s 103ms/step - loss: 0.6932 - accuracy: 0.5002 - val_loss: 0.6931 - val_accuracy: 0.5062\n",
      "Epoch 3/3\n",
      "625/625 [==============================] - 64s 102ms/step - loss: 0.6933 - accuracy: 0.4983 - val_loss: 0.6932 - val_accuracy: 0.4938\n",
      "[0.6931524276733398, 0.5]\n"
     ]
    }
   ],
   "source": [
    "_, cnn_weight_file = compile_and_train(model_v2, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a83a6d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем веса моделей\n",
    "CNN_WEIGHT_FILE = os.path.join(cnn_weight_file)\n",
    "LSTM_WEIGHT_FILE = os.path.join(lstm_weight_file)\n",
    "\n",
    "# Получаем модели\n",
    "cnn_model = cnn(inp)\n",
    "lstm_model = lstm(inp)\n",
    "\n",
    "# Загружаем сохраненные веса моделей\n",
    "cnn_model.load_weights(CNN_WEIGHT_FILE)\n",
    "lstm_model.load_weights(LSTM_WEIGHT_FILE)\n",
    "\n",
    "# Обьединяем модели в один массив\n",
    "models = [cnn_model, lstm_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95c5028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ansemble(models, model_input):\n",
    "    # Получаем результаты моделей\n",
    "    outputs = [model.outputs[0] for model in models]\n",
    "    # Усредняем\n",
    "    y = Average()(outputs)\n",
    "    # Записываем в одну модель усредненные результаты работы моделей\n",
    "    model = Model(model_input, y, name='ansemble')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d5cd168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "391/391 [==============================] - 344s 876ms/step - loss: 0.3956 - accuracy: 0.9010 - val_loss: 0.3716 - val_accuracy: 0.9247\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 348s 890ms/step - loss: 0.3279 - accuracy: 0.9122 - val_loss: 0.2166 - val_accuracy: 0.9282\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 358s 915ms/step - loss: 0.2301 - accuracy: 0.9161 - val_loss: 0.1791 - val_accuracy: 0.9357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19649949f90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Обучаем модель, полученную после ансамблирования моделей\n",
    "e_model = ansemble(models, inp)\n",
    "e_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "e_model.fit(np.asarray(x_train).astype('float32'),\n",
    "            np.asarray(y_train).astype('float32'),\n",
    "            validation_data=(np.asarray(x_test).astype('float32'), np.asarray(y_train).astype('float32')),\n",
    "            epochs=num_epochs, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8dbfc1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Получаем индексы из датасета\n",
    "indexes = imdb.get_word_index()\n",
    "\n",
    "# Функция, позволяющая загружать пользовательский текст и получать результат ансабля сетей\n",
    "def user_text_predict(text, model):\n",
    "    text_vec = np.zeros((1, max_review_length)) # Заполняем до max_review_length одномерный вектор\n",
    "    # Преобразуем текст\n",
    "    for idx, word in enumerate(text.translate(str.maketrans('', '', string.punctuation)).lower().split(' ')):\n",
    "        # Если индексы больше 10000 или None присваиваем 0\n",
    "        if idx < 500 and word in indexes:\n",
    "            text_vec[0][idx] = indexes[word]\n",
    "    tps = sequence.pad_sequences(text_vec, maxlen=max_review_length)\n",
    "    return np.around(model.predict(tps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c60645c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_text_predict('This film is disgusting', e_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39099aff",
   "metadata": {},
   "source": [
    "## Вывод\n",
    "\n",
    "В результате выполнения лабораторной работы было проведено ансамблирование моделей, благодаря которому значительно повысилась точность работы. Была написана функция, позволяющая загружать пользовательский текст и получать результат ансабля сетей, и провели тестирование сетей на основе загруженного текста."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
